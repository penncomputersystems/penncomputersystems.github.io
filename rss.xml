<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Blog by Penn Computer Systems]]></title><description><![CDATA[Penn Computer Systmes Blog]]></description><link>https://penncomputersystems.github.io</link><generator>GatsbyJS</generator><lastBuildDate>Mon, 22 Feb 2021 19:54:34 GMT</lastBuildDate><item><title><![CDATA[ZooKeeper: A Coordination Kernel]]></title><description><![CDATA[ZooKeeper is a coordination kernel that provides a simple API along with powerful guarantees that provides clients an easy way to build concurrency primitives for coordinating large-scale distributed processes.]]></description><link>https://penncomputersystems.github.io/posts/zookeeper-coordination-kernel</link><guid isPermaLink="false">https://penncomputersystems.github.io/posts/zookeeper-coordination-kernel</guid><pubDate>Thu, 18 Feb 2021 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;A key challenge commonly faced in distributed systems design has to do with
process coordination. Many large scale systems require efficient ways to coordinate
between servers, and there are variations on the purpose of coordination (e.g. node configuration,
group membership etc.) each with different requirements. Thus, developing a service
best suited for a particular application from scratch can be challenging and time consuming.&lt;/p&gt;
&lt;p&gt;Introducing ZooKeeper, first described in a
&lt;a href=&quot;https://www.usenix.org/legacy/event/atc10/tech/full_papers/Hunt.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;paper&lt;/a&gt;
published by Yahoo!, a service used for coordinating processes in distributed systems.
Authors of ZooKeeper call it a &lt;em&gt;coordination kernel&lt;/em&gt; as it exposes an interface
that allows clients to build coordination primitives (e.g. locks, condition variables)
without changes to the underlying service, much like an operating system provides
system calls for user-space processes. Contrast this with services that targets a specific use
case, such as the Akamai Configuration Management System for node configuration, or
the Amazon Simple Queue Service for queuing, ZooKeeper is much more flexible, which
allows clients to build arbitrary primitives for specific use cases.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Distributed systems are a zoo. They are chaotic and hard to manage, and ZooKeeper is meant to keep them
under control.”&lt;/p&gt;
&lt;div style=&quot;text-align: right&quot;&gt; - on the origin of the name &quot;ZooKeeper&quot; &lt;/div&gt;
&lt;/blockquote&gt;
&lt;p&gt;One of the advantages of ZooKeeper is the emphasis on being “wait-free”, meaning that
its implementation does not make use of blocking primitives, such as locks. This
avoids common problems where slow/faulty clients become the bottleneck in the system.
Yet surprisingly, this does not prevent the implementation of locks as we will see later on.
That being said, in order to be an effective service for coordination, ZooKeeper
must provide some reasonable guarantees.&lt;/p&gt;
&lt;p&gt;In this article, we will discuss the basic ZooKeeper API and its semantics,
the guarantees that come with ZooKeeper, some example implementations of common
primitives, as well as discuss the underlying implementation of ZooKeeper itself.&lt;/p&gt;
&lt;h1 id=&quot;the-zookeeper-service&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#the-zookeeper-service&quot; aria-label=&quot;the zookeeper service permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The ZooKeeper Service&lt;/h1&gt;
&lt;h2 id=&quot;the-abstraction&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#the-abstraction&quot; aria-label=&quot;the abstraction permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The Abstraction&lt;/h2&gt;
&lt;p&gt;The ZooKeeper service is usually composed of an ensemble of ZooKeeper servers.
Clients of the service (which are processes that you want to coordinate) connect
to the service by obtaining a session handle from a particular server, but the
handle itself persist across ZooKeeper servers, which allow clients to transparently
move from one server to another.&lt;/p&gt;
&lt;p&gt;The abstraction that ZooKeeper provides is a set of data nodes, organized
according to a hierarchical name space, which is a close analogue of
files in a typical filesystem. Each data node is stored in-memory, and can
contain arbitrary data, but is not designed for data storage, but more for
coordination metadata.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 960px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/f8d78043f7a4587b6e8d7604f4d00c9b/33d1d/example-namespace.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 53.75%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAABUElEQVQoz3VT266CQAzk/z/smBwfPCo37yaC+iCgIuAFndNZWWQhNmm26W6HzrRY+GKv10u5jmlFUaB8PIycPrVZX4Gq8/l8qnwSx/B9H7PpzHjXBrZMoE+sgbSFQQjbtjEaDlWnzTq+7QBW12gyyPMcSZIgTVM8hGqWZbjdbigkz9zpdOqws8z23/Fuu8V6tcZEKI6GI9zv947GQRDAdV2RYIrVconLJft0aOggfr1eEYYhPCmwx2Ocz2d1T2paiv1uJ/celouFgF0Ug46G+kwrAFLMxfkBxu231LVDuTmpzWYD13FQlqX64mI+V7l3R3s4tlMPhDXH47GzYgZlR8D6v30FxoH8DQZqskozAe799BAdDjX9KIqM9ak7pJGW73lKYALG8tj3fAXEYmpK8Fj2UQ+JNe3lrgFJM6v0IiDXg11yRQjIPJ2Umzva/lP+AXklVshW1h2DAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
          &lt;source
              srcset=&quot;/static/f8d78043f7a4587b6e8d7604f4d00c9b/8ac56/example-namespace.webp 240w,
/static/f8d78043f7a4587b6e8d7604f4d00c9b/d3be9/example-namespace.webp 480w,
/static/f8d78043f7a4587b6e8d7604f4d00c9b/e46b2/example-namespace.webp 960w,
/static/f8d78043f7a4587b6e8d7604f4d00c9b/7ed5b/example-namespace.webp 1150w&quot;
              sizes=&quot;(max-width: 960px) 100vw, 960px&quot;
              type=&quot;image/webp&quot;
            /&gt;
          &lt;source
            srcset=&quot;/static/f8d78043f7a4587b6e8d7604f4d00c9b/8ff5a/example-namespace.png 240w,
/static/f8d78043f7a4587b6e8d7604f4d00c9b/e85cb/example-namespace.png 480w,
/static/f8d78043f7a4587b6e8d7604f4d00c9b/d9199/example-namespace.png 960w,
/static/f8d78043f7a4587b6e8d7604f4d00c9b/33d1d/example-namespace.png 1150w&quot;
            sizes=&quot;(max-width: 960px) 100vw, 960px&quot;
            type=&quot;image/png&quot;
          /&gt;
          &lt;img
            class=&quot;gatsby-resp-image-image&quot;
            src=&quot;/static/f8d78043f7a4587b6e8d7604f4d00c9b/d9199/example-namespace.png&quot;
            alt=&quot;Illustration of an example of ZooKeeper state&quot;
            title=&quot;Illustration of an example of ZooKeeper state&quot;
            loading=&quot;lazy&quot;
            style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
          /&gt;
        &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In particular, there are two types of data nodes that can be created:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Regular&lt;/strong&gt;: Clients create and delete regular nodes explictly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ephemeral&lt;/strong&gt;: Clients create ephemeral nodes and can delete
them explicitly, but the system can remove such nodes automatically
when the session that created the node terminates.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that ephemeral nodes can be useful for implementing certain primitives that are
dependent on whether specific nodes are connected (e.g. leader election,
group membership).&lt;/p&gt;
&lt;p&gt;Further, each data node can be created with the &lt;em&gt;sequential&lt;/em&gt; flag. Nodes
created with the flag will have a monotonically increasing counter assigned
and appended to its name. This can be useful for implementing primitives
that require queuing.&lt;/p&gt;
&lt;p&gt;Finally, ZooKeeper also provides a watching facility, much like
subscription services/change streams, which allows clients to
receive a notification of a change without polling. This can be useful
for implementing primitives that require event waiting or callbacks.&lt;/p&gt;
&lt;h2 id=&quot;client-api&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#client-api&quot; aria-label=&quot;client api permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Client API&lt;/h2&gt;
&lt;p&gt;Clients of the ZooKeeper service issue requests through the client API. We present
a basic subset of the API discussed in the paper, which we will use to build
some primitives later on.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;create(path, data, flags)&lt;/code&gt;: Creates a data node with path name &lt;code class=&quot;language-text&quot;&gt;path&lt;/code&gt;,
stores &lt;code class=&quot;language-text&quot;&gt;data[]&lt;/code&gt; and returns the name of the data node. &lt;code class=&quot;language-text&quot;&gt;flags&lt;/code&gt; allow
a client to select between regular and ephemeral data nodes, as well as make
the data node sequential.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;delete(path, version)&lt;/code&gt;: Deletes the data node at &lt;code class=&quot;language-text&quot;&gt;path&lt;/code&gt; if the version
number matches the data node.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;exists(path, watch)&lt;/code&gt;: Returns whether a data node at &lt;code class=&quot;language-text&quot;&gt;path&lt;/code&gt; exists. The
&lt;code class=&quot;language-text&quot;&gt;watch&lt;/code&gt; flag enables the client to watch the data node for changes.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;getData(path, watch)&lt;/code&gt;: Returns the data associated with the data node. The
&lt;code class=&quot;language-text&quot;&gt;watch&lt;/code&gt; flag enables the client to watch the data node for changes.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;setData(path, data, version)&lt;/code&gt;: Writes &lt;code class=&quot;language-text&quot;&gt;data[]&lt;/code&gt; to the data node at &lt;code class=&quot;language-text&quot;&gt;path&lt;/code&gt;
if the version number matches the data node.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;getChildren(path, watch)&lt;/code&gt;: Returns the set of children names of a data node.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;sync(path)&lt;/code&gt;: Waits for all updates pending at the start of the operation
to propagate to the server that the client is connected to.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that these requests can be issued asynchronously. In particular, &lt;em&gt;multiple&lt;/em&gt;
requests can be issued from the same client at a time.&lt;/p&gt;
&lt;h2 id=&quot;zookeeper-guarantees&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#zookeeper-guarantees&quot; aria-label=&quot;zookeeper guarantees permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ZooKeeper Guarantees&lt;/h2&gt;
&lt;p&gt;A coordination kernel would be next to useless without reasonable guarantees about
how it behaves when operations interleave. Thus, ZooKeeper provides the following
guarantees.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linearizable writes&lt;/strong&gt;: All requests that update the global state of the
ZooKeeper ensemble are serializable and respect precedence. This means that the
outcome of a set of possibly interleaving writes is equal to the outcome of
those writes executing serially. Further, write requests are always processed in
order.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FIFO client order&lt;/strong&gt;: All requests from a given client are executed
in the order that they were sent by the client.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that ZooKeeper was designed for read-heavy workloads (target read-to-write ratios
range from 2:1 to 100:1), thus ZooKeeper does &lt;strong&gt;not&lt;/strong&gt; support linearizable reads!
In other words, reads from a given server can be stale, meaning that updates
that are committed to global state might not be visible. This is a
conscious design decision and does not affect the correctness of the primitives
we can build with ZooKeeper if done carefully. We will discuss why this occurs
when we discuss the implementation details of the ZooKeeper service later on.&lt;/p&gt;
&lt;h1 id=&quot;basic-coordination-primitives&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#basic-coordination-primitives&quot; aria-label=&quot;basic coordination primitives permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Basic Coordination Primitives&lt;/h1&gt;
&lt;p&gt;Having seen the abstraction, client API and guarantees of ZooKeeper, let us
put it in action and build a couple of simple coordination primitives with ZooKeeper,
namely read-write locks and leader election.&lt;/p&gt;
&lt;h2 id=&quot;readwrite-locks&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#readwrite-locks&quot; aria-label=&quot;readwrite locks permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Read/Write Locks&lt;/h2&gt;
&lt;p&gt;A read-write lock is a locking primitive allows multiple readers to hold the lock
&lt;strong&gt;or&lt;/strong&gt; exactly one writer to hold the lock at one point in time. To build such
a primitive, we create a regular data node at path &lt;code class=&quot;language-text&quot;&gt;l&lt;/code&gt; to hold metadata for
a particular lock. Then, for each client wishing to hold the lock for reading or
writing, we line them up by creating a &lt;em&gt;sequential&lt;/em&gt; data node, and watch the
data node ordered just in front of the newly created node for changes.
The following pseudocode describes the logic in more detail.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Write Lock&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;1 n = create(l + “/write-”, EPHEMERAL|SEQUENTIAL)
2 C = getChildren(l, false)
3 if n is lowest data node in C, exit
4 p = data node in C ordered just before n
5 if exists(p, true) wait for event
6 goto 2&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Read Lock&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;1 n = create(l + “/read-”, EPHEMERAL|SEQUENTIAL)
2 C = getChildren(l, false)
3 if no write data nodes lower than n in C, exit
4 p = write data node in C ordered just before n
5 if exists(p, true) wait for event
6 goto 3&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Unlock&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;1 delete(n)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A couple of interesting points to note:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;em&gt;sequential&lt;/em&gt; flag came in useful to implement queuing for locks. Since reads
can easily be stale, allowing the server to choose the concrete data node path
prevents race conditions that typically happen due to the lack of atomicity
of a read followed by a write.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;ephemeral&lt;/em&gt; flag made sure that clients that disconnect due to a failure or
without releasing the lock on exit will not hold the lock indefinitely.&lt;/li&gt;
&lt;li&gt;The familiar loop to check whether a lock has been obtained by the client is
important, since it is possible that the previous lock request in queue was abandoned
(say by a disconnection) while an earlier client is still holding the lock.&lt;/li&gt;
&lt;li&gt;The watch facility provided by ZooKeeper is a neat way to notify a client that
they can attempt to obtain the lock without polling.&lt;/li&gt;
&lt;li&gt;There is no &lt;a href=&quot;https://en.wikipedia.org/wiki/Thundering_herd_problem&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;herd effect&lt;/a&gt;
since exactly one client is woken up when a lock is released.&lt;/li&gt;
&lt;li&gt;Even though ZooKeeper does not support linearizable reads, we can be confident that the list
of children &lt;code class=&quot;language-text&quot;&gt;C&lt;/code&gt; obtained above does not miss any clients that came before, since it is preceded
by a write operation.
If the write operation resolves, the read operation that follows must be as recent
as the state right after the write operation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;leader-election&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#leader-election&quot; aria-label=&quot;leader election permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Leader Election&lt;/h2&gt;
&lt;p&gt;Leader elections are commonly used to coordinate between multiple processes
in distributed systems, where a leader is “elected” to make decisions which
“followers” obey. &lt;a href=&quot;https://penncomputersystems.github.io/posts/raft-consensus&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Raft&lt;/a&gt;
makes use of leader elections for consistent log replication, and ZooKeeper
itself follows a leader replication model, as we will see later.&lt;/p&gt;
&lt;p&gt;To build such a primitive, we create a regular data node at path &lt;code class=&quot;language-text&quot;&gt;l&lt;/code&gt; to
hold metadata for the elction. Then, we again create a &lt;em&gt;sequential&lt;/em&gt; node
at &lt;code class=&quot;language-text&quot;&gt;l&lt;/code&gt;, and watch for changes of the preceding data node, and assume the role
of leader if it ever gets deleted. The following pseudocode describes the
idea more concretely.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Election&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;1 n = create(l + &amp;quot;/n-&amp;quot;, EPHEMEREAL|SEQUENTIAL)
2 C = getChildren(l, false)
3 if n is lowest data node in C, assume leader, exit
4 p = data node in C ordered just before n
5 if exists(p, true) wait for event
6 goto 2&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The underlying idea for implementing leader election is similar to how
we implemented the read/write lock above. Can you spot how we avoided the
herd effect? Which guarantees of ZooKeeper prevent two clients from thinking
they are both the leaders?&lt;/p&gt;
&lt;h1 id=&quot;zookeeper-implementation&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#zookeeper-implementation&quot; aria-label=&quot;zookeeper implementation permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ZooKeeper Implementation&lt;/h1&gt;
&lt;p&gt;Finally, we discuss some of the interesting aspects of the implementation of
ZooKeeper itself, namely how it provides its guarantees, and how it is optimized
for real-world throughputs.&lt;/p&gt;
&lt;h2 id=&quot;replicated-databases&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#replicated-databases&quot; aria-label=&quot;replicated databases permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Replicated Databases&lt;/h2&gt;
&lt;p&gt;As discussed, a ZooKeeper service is usually composed of multiple servers
working as an ensemble. This allows ZooKeeper to provide high availability
by replicating the data nodes on each server. In order to maintain consistency
between servers, ZooKeeper opts for a common strategy for horizontal scaling,
namely leader replication. More specifically, a server is selected as the
&lt;em&gt;leader&lt;/em&gt; and the other servers are &lt;em&gt;followers&lt;/em&gt;. Further, writes are always
routed through the reader and commits when there is a quorum. Note
that reads on the other hand can be served by a local replica which may be
out-of-date and stale (which is why ZooKeeper does not support linearizable reads),
but this allows for higher read throughput.&lt;/p&gt;
&lt;h2 id=&quot;zab-an-atomic-broadcast-protocol&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#zab-an-atomic-broadcast-protocol&quot; aria-label=&quot;zab an atomic broadcast protocol permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Zab: An Atomic Broadcast Protocol&lt;/h2&gt;
&lt;p&gt;ZooKeeper makes use of &lt;em&gt;Zab&lt;/em&gt;, an atomic broadcast protocol to provide
the strong guarantees as described previously. At a high level, Zab
makes use of a majority quorum to commit a proposal to a state change,
and guarantees that changes are delivered in order that they were sent
and are atomic. ZooKeeper also makes use of &lt;em&gt;Zab&lt;/em&gt; for fault tolerance,
and uses the protocol to deliver “catch-up” messages since the last
snapshot in the event of a server failure.&lt;/p&gt;
&lt;p&gt;An important point about Zab is that each state change is recorded
in an &lt;em&gt;idempotent&lt;/em&gt; transaction. In other words, applying a particular
transaction multiple times will not affect the final state of the system.
Note that this often requires the leader to execute client requests locally
before proposing a change. For instance, an operation to increment the value
of the data node at &lt;code class=&quot;language-text&quot;&gt;path&lt;/code&gt; has to be converted to setting the value of the
data node to some value to preserve idempotency.&lt;/p&gt;
&lt;p&gt;For more details and theoretical proofs of correctness of &lt;em&gt;Zab&lt;/em&gt;, check out
the paper &lt;a href=&quot;https://marcoserafini.github.io/papers/zab.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;fuzzy-snapshots&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#fuzzy-snapshots&quot; aria-label=&quot;fuzzy snapshots permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fuzzy Snapshots&lt;/h2&gt;
&lt;p&gt;Since the data nodes are stored in-memory, each replica has to take snapshots
periodically as replaying all transactions from the leader would potentially
be too time consuming. Recall that ZooKeeper is &lt;em&gt;wait-free&lt;/em&gt;, so the snapshots
are &lt;em&gt;fuzzy snapshots&lt;/em&gt;, since the implementation does not take a lock on the
state of a particular replica. Interestingly, a resulting snapshot may not correspond
to the state of ZooKeeper at any point in time (can you think of a series of
transactions and snapshot procedure that results in this behavior?). But,
since transactions are idempotent, a replica can easily apply them as long as
the transactions are stored in the correct order.&lt;/p&gt;
&lt;h1 id=&quot;summary&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#summary&quot; aria-label=&quot;summary permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Summary&lt;/h1&gt;
&lt;p&gt;ZooKeeper was initially developed at Yahoo! and used for a variety of Yahoo!
services such as the search engine crawler and their distributed pub-sub system,
but has seen many more use cases ever since. For instance, the Apache Software
Foundation has developed &lt;a href=&quot;https://zookeeper.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Apache ZooKeeper&lt;/a&gt;
as an open-source version of ZooKeeper and is widely used for many production
applications including Apache Hadoop and Kafka as well as companies including
Yelp, Reddit, Facebook, Twitter etc. Its simple interface, reasonable
consistency guarantees, and flexible abstractions provides the user with the
ability to tailor specific primitives that fit the requirements of a particular system.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;P.S.&lt;/em&gt; Personally, I first heard about ZooKeeper from an interesting
&lt;a href=&quot;https://www.youtube.com/watch?v=mMuk8Rn9HBg&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Youtube video&lt;/a&gt; from one of my favorite programming channels
on building an asynchronous ZooKeeper client in Rust, which is in
my opinion a great way to get hands-on experience with interacting with ZooKeeper!&lt;/p&gt;</content:encoded></item><item><title><![CDATA[The RAFT Consensus Algorithm]]></title><description><![CDATA[The RAFT consensus protocol establishes a method for distributed consensus in a cluster of machines, by managing a replicated log. RAFT cleanly separates the key elements of consensus: leader election, failure recovery and safety and thus is a lot easier to understand than Paxos.]]></description><link>https://penncomputersystems.github.io/posts/raft-consensus</link><guid isPermaLink="false">https://penncomputersystems.github.io/posts/raft-consensus</guid><pubDate>Sun, 31 Jan 2021 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Consensus algorithms hold a very important place in distributed systems design, allowing a cluster of machines to work as a coherent group with failure transparency (i.e. the end user does not know when individual machines in the cluster have failed as long as some are alive). The easiest application to visualize is a replicated key-value store, and we’ll use this to make the discussion on RAFT concrete. &lt;/p&gt;
&lt;p&gt;RAFT is an algorithm that allows a cluster of machines to act as a &lt;a href=&quot;https://en.wikipedia.org/wiki/State_machine_replication&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;replicated state machine&lt;/a&gt; which essentially means that each machine has identical state and changes to that state are carried out in the same order on all the machines. The state for a key-value store will be the key-value pairs, and a replicated state machine means that all the writes(creation and deletion) occur in the same order. This is very useful, because if one of the machines goes down, we have a bunch of other machines with identical state. RAFT does this by managing a replicated log of operations. So in the case of the key-value store, it would look something like:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;PUT key:ComputerSystems value:Article1
PUT key:ComputerSystems value:Article2
DELETE key:ComputerSystems&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If all machines in the cluster maintain this log, each machine agrees on the order of log operations and applies the operation to its internal state, we will have a replicated state machine! The challenge is maintenance of the log: how do we ensure that all the machines agree on the order of these log operations in the face of network failures, network latencies, machine failures, and other adversarial conditions. This is what RAFT aims to solve. &lt;/p&gt;
&lt;h2 id=&quot;a-deeper-dive-into-raft&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#a-deeper-dive-into-raft&quot; aria-label=&quot;a deeper dive into raft permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A Deeper Dive into RAFT&lt;/h2&gt;
&lt;p&gt;RAFT implements consensus by first electing a distinguished &lt;em&gt;leader&lt;/em&gt; that accepts operations from &lt;em&gt;clients&lt;/em&gt;. Clients in this case will be users of the key-value store: if I want to store a value, I’ll connect to the leader over the network and send my request. RAFT does this because it simplifies management of the replicated log. Given this model, consensus is broken up into three independent subproblems: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Leader Election&lt;/strong&gt;: how do we choose the leader when the system starts, or when we detect that the current leader has failed?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Log Replication&lt;/strong&gt;: how does the leader then ensure that the log entries are replicated on all the machines?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Safety&lt;/strong&gt;: how can we ensure that all the machines apply the logs in the same order? In other words, if any machine applies a log entry at a particular index, no server should apply a different entry to that same index. &lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;raft-basics&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#raft-basics&quot; aria-label=&quot;raft basics permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;RAFT Basics&lt;/h3&gt;
&lt;p&gt;RAFT clusters are made up of an odd number of machines, and a cluster with &lt;em&gt;2n+1&lt;/em&gt; machines can tolerate &lt;em&gt;n&lt;/em&gt; failures, because we want a majority of the machines to be alive at any given time. At any given time, a machine is either a &lt;em&gt;leader&lt;/em&gt;, a &lt;em&gt;follower&lt;/em&gt; or a &lt;em&gt;candidate&lt;/em&gt;. During normal operation, there is exactly one leader and all the other servers are followers, just responding to the leader’s requests. The leader is responsible for client requests and also for maintaining the log. The candidate state occurs in leader election, where this state signifies that the server is a possible contender for the next leader. &lt;/p&gt;
&lt;p&gt;RAFT operation is divided into &lt;em&gt;terms&lt;/em&gt; of arbitrary length, numbered with consecutive integers. Each term begins with an &lt;em&gt;election&lt;/em&gt;, where the leader for that term is decided. If there is a split vote for leader election, then there will be a randomized backoff, and leader election will take place again. Thus, RAFT ensures that there is only one leader in any given term. The following figure, taken from the paper, illustrates this:
&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 852px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/d095083ae8008c2e9bfd52da67dc3798/16274/raft-terms.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 30.41666666666667%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAGABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAIDBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHbiJcF/8QAFxABAAMAAAAAAAAAAAAAAAAAAAIRIf/aAAgBAQABBQKStf/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABcQAAMBAAAAAAAAAAAAAAAAAAABMRD/2gAIAQEABj8CpXn/xAAZEAADAQEBAAAAAAAAAAAAAAAAARExUXH/2gAIAQEAAT8h7peFuckLD//aAAwDAQACAAMAAAAQd8//xAAWEQADAAAAAAAAAAAAAAAAAAABEBH/2gAIAQMBAT8QgX//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAaEAEBAAIDAAAAAAAAAAAAAAABEQAhMUFh/9oACAEBAAE/ECiSgkXuaEeyXTlAvPef/9k=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
          &lt;source
              srcset=&quot;/static/d095083ae8008c2e9bfd52da67dc3798/8ac56/raft-terms.webp 240w,
/static/d095083ae8008c2e9bfd52da67dc3798/d3be9/raft-terms.webp 480w,
/static/d095083ae8008c2e9bfd52da67dc3798/39392/raft-terms.webp 852w&quot;
              sizes=&quot;(max-width: 852px) 100vw, 852px&quot;
              type=&quot;image/webp&quot;
            /&gt;
          &lt;source
            srcset=&quot;/static/d095083ae8008c2e9bfd52da67dc3798/09b79/raft-terms.jpg 240w,
/static/d095083ae8008c2e9bfd52da67dc3798/7cc5e/raft-terms.jpg 480w,
/static/d095083ae8008c2e9bfd52da67dc3798/16274/raft-terms.jpg 852w&quot;
            sizes=&quot;(max-width: 852px) 100vw, 852px&quot;
            type=&quot;image/jpeg&quot;
          /&gt;
          &lt;img
            class=&quot;gatsby-resp-image-image&quot;
            src=&quot;/static/d095083ae8008c2e9bfd52da67dc3798/16274/raft-terms.jpg&quot;
            alt=&quot;Raft terms&quot;
            title=&quot;Raft terms&quot;
            loading=&quot;lazy&quot;
            style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
          /&gt;
        &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;These term numbers are important because they act as a logical clock for the system. Some servers (if in a network partition, or other cases) may miss complete terms. The term numbers helps the machines detect stale leaders and old information. &lt;/p&gt;
&lt;p&gt;Communication in this system happens via &lt;a href=&quot;https://en.wikipedia.org/wiki/Remote_procedure_call&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Remote Procedure Calls&lt;/a&gt; (RPCs). This is essentially an abstraction over a traditional TCP connection that allows a server to call a function on another server. One can view it as a simple function call, but where the function executes on another server rather than on the server making the call. The system contains only 2 main RPCs, the &lt;strong&gt;AppendEntries&lt;/strong&gt; RPC and the &lt;strong&gt;RequestVotes&lt;/strong&gt; RPC.
We will now look into the first problem of leader election, and how RAFT does this. &lt;/p&gt;
&lt;h3 id=&quot;leader-election-in-raft&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#leader-election-in-raft&quot; aria-label=&quot;leader election in raft permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Leader Election in RAFT&lt;/h3&gt;
&lt;p&gt;When servers start up, they are all in the &lt;em&gt;follower&lt;/em&gt; state. Leader election happens with heartbeats, which is an important concept in distributed systems. Heartbeats are when servers send out periodic messages to simply indicate that they are alive. If a server sees a certain period of time without a heartbeat, it will assume that the leader has died, and then will transition to the &lt;em&gt;candidate&lt;/em&gt; state. The threshold of time before the follower concludes that the leader is dead is called an &lt;em&gt;election timeout&lt;/em&gt;. Note that these election timeouts are randomized per server, and so they won’t all transition into candidacy at the same time.&lt;/p&gt;
&lt;p&gt;When a &lt;em&gt;follower&lt;/em&gt; becomes a &lt;em&gt;candidate&lt;/em&gt;, it votes for itself and then issues &lt;strong&gt;RequestVote&lt;/strong&gt; RPCs to all the other servers in the cluster. A candidate will become a leader if it receives votes from a majority of the servers in the cluster. It also may be the case where there is a race: two servers transition into candidacy at about the same time. Since we are using a majority vote and each server can only vote for one server, only one can win the race. Thus, if a server is in the candidate stage and it recieves an AppendEntries RPC from another server with a term number that is atleast as large as its own term number, it will recognize that another server has won the race and will transition back to the follower state. &lt;/p&gt;
&lt;p&gt;It may also be the case that there is a split vote, if many followers become candidates at the same time. IN this case, the candidate will time-out and then start a new election by incrementing its term and starting a new round of &lt;strong&gt;RequestVote&lt;/strong&gt; RPCs. To prevent a livelock (where the split votes occur indefinitely), RAFT adds randomization to the election timeouts, and they are randomly chose from a fixed interval (150ms-300ms). This randomization also means that on failure of the leader, only one of the server’s will time-out, and so there will be only one server requesting votes reducing the chance of a split vote. Note that the 150ms-300ms range comes from experimentation, and ensures that network delays in the leader’s heartbeats do not frequently trigger new leader elections while the current leader is alive.&lt;/p&gt;
&lt;p&gt;See the figure (taken from the paper) below for an explanation of the mechanism of the &lt;strong&gt;RequestVote&lt;/strong&gt; RPC.&lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 814px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/623ead13a6db6b7f86cbc406bea863e5/e51eb/raft-reqvotes.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 75.83333333333333%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAgABBf/EABUBAQEAAAAAAAAAAAAAAAAAAAEA/9oADAMBAAIQAxAAAAHqpagnN//EABgQAAMBAQAAAAAAAAAAAAAAAAABAhIg/9oACAEBAAEFAlEmIMSuP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABYRAAMAAAAAAAAAAAAAAAAAAAEQEf/aAAgBAgEBPwGlf//EABgQAAIDAAAAAAAAAAAAAAAAAAAyASAh/9oACAEBAAY/AlFMin//xAAZEAADAQEBAAAAAAAAAAAAAAAAARExUZH/2gAIAQEAAT8hrb2i2nmUWYXt9G6f/9oADAMBAAIAAwAAABCnz//EABYRAAMAAAAAAAAAAAAAAAAAAAABEf/aAAgBAwEBPxCIiP/EABYRAQEBAAAAAAAAAAAAAAAAAAABEf/aAAgBAgEBPxAbX//EABwQAQACAwADAAAAAAAAAAAAAAEAIRExcVGBof/aAAgBAQABPxBbhY5u/sAYcPc0SS8XBZip3DxLigaCdn//2Q==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
          &lt;source
              srcset=&quot;/static/623ead13a6db6b7f86cbc406bea863e5/8ac56/raft-reqvotes.webp 240w,
/static/623ead13a6db6b7f86cbc406bea863e5/d3be9/raft-reqvotes.webp 480w,
/static/623ead13a6db6b7f86cbc406bea863e5/f23e7/raft-reqvotes.webp 814w&quot;
              sizes=&quot;(max-width: 814px) 100vw, 814px&quot;
              type=&quot;image/webp&quot;
            /&gt;
          &lt;source
            srcset=&quot;/static/623ead13a6db6b7f86cbc406bea863e5/09b79/raft-reqvotes.jpg 240w,
/static/623ead13a6db6b7f86cbc406bea863e5/7cc5e/raft-reqvotes.jpg 480w,
/static/623ead13a6db6b7f86cbc406bea863e5/e51eb/raft-reqvotes.jpg 814w&quot;
            sizes=&quot;(max-width: 814px) 100vw, 814px&quot;
            type=&quot;image/jpeg&quot;
          /&gt;
          &lt;img
            class=&quot;gatsby-resp-image-image&quot;
            src=&quot;/static/623ead13a6db6b7f86cbc406bea863e5/e51eb/raft-reqvotes.jpg&quot;
            alt=&quot;Request Votes RPC&quot;
            title=&quot;Request Votes RPC&quot;
            loading=&quot;lazy&quot;
            style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
          /&gt;
        &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;log-replication-in-raft&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#log-replication-in-raft&quot; aria-label=&quot;log replication in raft permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Log Replication in RAFT&lt;/h3&gt;
&lt;p&gt;Once a leader has been elected, it begins servicing client requests. Each request contains a command to be executed by the replicated state machines. The leader appends this command to its own log, and then issues &lt;strong&gt;AppendEntries&lt;/strong&gt; RPCs in parallel to all the servers in the cluster (spins up a thread to send &lt;strong&gt;AppendEntries&lt;/strong&gt; RPCs). If the follower doesn’t respond due to a crash or loss of network packets, the &lt;strong&gt;AppendEntries&lt;/strong&gt; RPC will be retried indefinitely, even after responding to the client. Logs are organized as follows, with each entry storing a state machine command, along with the term number. In the figure below (taken from the paper), the state consists of two variables &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt;, whose values are updated. &lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 798px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/c2bd5dd81f6ede010e5477d84b3d1479/aad24/raft-log.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 67.91666666666667%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAOABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAMBAgX/xAAUAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHTvLAYAv8A/8QAGRAAAgMBAAAAAAAAAAAAAAAAAAECEBEh/9oACAEBAAEFAtZF12Itr//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABURAQEAAAAAAAAAAAAAAAAAAAEQ/9oACAECAQE/ASf/xAAZEAACAwEAAAAAAAAAAAAAAAAAARAhMTL/2gAIAQEABj8C06caW4//xAAbEAACAgMBAAAAAAAAAAAAAAABIQAREDFhUf/aAAgBAQABPyHc0BYYfINKAaQfJeEG+Y//2gAMAwEAAgADAAAAEFjP/8QAFREBAQAAAAAAAAAAAAAAAAAAARD/2gAIAQMBAT8QBn//xAAWEQEBAQAAAAAAAAAAAAAAAAAhARD/2gAIAQIBAT8QEc//xAAbEAEAAgMBAQAAAAAAAAAAAAABABEhMUFRsf/aAAgBAQABPxAv8D2N17uY+QhRWRZOj1DcIGKpUANFT//Z&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
          &lt;source
              srcset=&quot;/static/c2bd5dd81f6ede010e5477d84b3d1479/8ac56/raft-log.webp 240w,
/static/c2bd5dd81f6ede010e5477d84b3d1479/d3be9/raft-log.webp 480w,
/static/c2bd5dd81f6ede010e5477d84b3d1479/ce206/raft-log.webp 798w&quot;
              sizes=&quot;(max-width: 798px) 100vw, 798px&quot;
              type=&quot;image/webp&quot;
            /&gt;
          &lt;source
            srcset=&quot;/static/c2bd5dd81f6ede010e5477d84b3d1479/09b79/raft-log.jpg 240w,
/static/c2bd5dd81f6ede010e5477d84b3d1479/7cc5e/raft-log.jpg 480w,
/static/c2bd5dd81f6ede010e5477d84b3d1479/aad24/raft-log.jpg 798w&quot;
            sizes=&quot;(max-width: 798px) 100vw, 798px&quot;
            type=&quot;image/jpeg&quot;
          /&gt;
          &lt;img
            class=&quot;gatsby-resp-image-image&quot;
            src=&quot;/static/c2bd5dd81f6ede010e5477d84b3d1479/aad24/raft-log.jpg&quot;
            alt=&quot;Log layout in RAFT&quot;
            title=&quot;Log layout in RAFT&quot;
            loading=&quot;lazy&quot;
            style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
          /&gt;
        &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Entries need to be &lt;em&gt;committed&lt;/em&gt; to the log, they cannot executed as soon as they’re received, because RAFT has to guarantee that this entry will be durable. In other words, if this log entry does not find its way onto a majority of the servers, no one should apply it to the state, as this risks divergence. The leader commits an entry, i.e. allows it to be applied to the state machine when a majority of servers have appended it to their local logs. A committing of a particular log index, also commits all previous entries not yet committed. The leader keeps track of the highest entry it has committed, and then sends that in the &lt;strong&gt;AppendEntries&lt;/strong&gt; RPCs for the other servers to eventually find out. The &lt;strong&gt;AppendEntries&lt;/strong&gt; RPC works as shown in the figure (taken from the paper) below. &lt;/p&gt;
&lt;p&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 802px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/84bc0d460784d23b4ded6c602d4a2289/4a8d4/raft-appendentries.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 125.83333333333333%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAZABQDASIAAhEBAxEB/8QAGAABAQEBAQAAAAAAAAAAAAAAAgABAwX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAAB9ZPLBdIOjYcav//EABsQAAIBBQAAAAAAAAAAAAAAAAACIAEDEhNB/9oACAEBAAEFAqW0MENaQ7H/xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/AR//xAAWEQEBAQAAAAAAAAAAAAAAAAARABD/2gAIAQIBAT8BZ3//xAAcEAABAwUAAAAAAAAAAAAAAAAAARAyAiAhMUH/2gAIAQEABj8CiRMUo3X3b//EAB0QAQEAAgEFAAAAAAAAAAAAAAEAESExYZGhwfD/2gAIAQEAAT8hyFx7Wc342D0INLaj9mSDiPWeY5Jv/9oADAMBAAIAAwAAABAE4v7/xAAXEQADAQAAAAAAAAAAAAAAAAAAAREQ/9oACAEDAQE/EIiIuf/EABYRAQEBAAAAAAAAAAAAAAAAAAARUf/aAAgBAgEBPxCtVqI//8QAHhABAAMAAgIDAAAAAAAAAAAAAQARITFREGFBcaH/2gAIAQEAAT8Qtk0t9pnDTDnEOxW4q+jWAW69MnxxM+0LHGEZFY62FstteL9U5T//2Q==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;picture&gt;
          &lt;source
              srcset=&quot;/static/84bc0d460784d23b4ded6c602d4a2289/8ac56/raft-appendentries.webp 240w,
/static/84bc0d460784d23b4ded6c602d4a2289/d3be9/raft-appendentries.webp 480w,
/static/84bc0d460784d23b4ded6c602d4a2289/85811/raft-appendentries.webp 802w&quot;
              sizes=&quot;(max-width: 802px) 100vw, 802px&quot;
              type=&quot;image/webp&quot;
            /&gt;
          &lt;source
            srcset=&quot;/static/84bc0d460784d23b4ded6c602d4a2289/09b79/raft-appendentries.jpg 240w,
/static/84bc0d460784d23b4ded6c602d4a2289/7cc5e/raft-appendentries.jpg 480w,
/static/84bc0d460784d23b4ded6c602d4a2289/4a8d4/raft-appendentries.jpg 802w&quot;
            sizes=&quot;(max-width: 802px) 100vw, 802px&quot;
            type=&quot;image/jpeg&quot;
          /&gt;
          &lt;img
            class=&quot;gatsby-resp-image-image&quot;
            src=&quot;/static/84bc0d460784d23b4ded6c602d4a2289/4a8d4/raft-appendentries.jpg&quot;
            alt=&quot;Append Entries RPC&quot;
            title=&quot;Append Entries RPC&quot;
            loading=&quot;lazy&quot;
            style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
          /&gt;
        &lt;/picture&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This RPC was designed to maintain a high degree of coherence between logs on different servers. In particular, RAFT maintains the Log Matching Property, which states that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If two entries in different logs have the same index and term, then they store the same command.&lt;/li&gt;
&lt;li&gt;If two entries in different logs have the same index and term, then the logs are identical in all preceding entries. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first property follows from the fact that a leader creates at most one entry with a given log index in a given term, and log entries never change position. We will dive into this deeper while looking at some failure scenarios. The second property follows from the simple consistency check performed by the &lt;strong&gt;AppendEntries&lt;/strong&gt; RPC. As shown above, arguments to the &lt;strong&gt;AppendEntries&lt;/strong&gt; RPC include &lt;code class=&quot;language-text&quot;&gt;prevLogIndex&lt;/code&gt; and &lt;code class=&quot;language-text&quot;&gt;prevLogTerm&lt;/code&gt; which are the log index and term of the entry immediately preceding the entries sent. If the follower’s log does not match this, it refuses the new entries. This acts as an induction step, always ensuring that the logs agree. &lt;/p&gt;
&lt;p&gt;In the absence of any failures, there will be no case where the logs disagree. However, crashes can leave logs inconsistent. A follower might crash and come back up after some log entries have already been committed (remember, the system will continue to function if a majority of the servers are up), and thus might be missing some entries. The follower might also have extra entries not present on the current leader. To test your understanding, try to think of a scenario where a follower in a given term can have extra entries that are not present in the leader’s log (hint : what happens if the leader in the preceding term receives entries by crashes before it can replicate them). RAFT handles the divergence in logs in a given term by forcing the follower’s logs to agree with the leader’s log. This means that any entries not on the leader’s log will be overwritten or deleted. This occurs through the &lt;strong&gt;AppendEntries&lt;/strong&gt; RPC. Notice that if the follower’s log does not agree with the leader’s log, the &lt;strong&gt;AppendEntries&lt;/strong&gt; consistency check will fail, and the new entries will get rejected. Once this happens, the leader will decrement the &lt;code class=&quot;language-text&quot;&gt;prevLogIndex&lt;/code&gt; and send the entry at the new &lt;code class=&quot;language-text&quot;&gt;prevLogIndex&lt;/code&gt;. This will continue until either there is a match, or the follower is forced to overwrite its log completely with the entire log of the leader. This is a clever method because the leader does not have to take any additional or special measures to ensure log consistency - the &lt;strong&gt;AppendEntries&lt;/strong&gt; RPC will take care of that. &lt;/p&gt;
&lt;h3 id=&quot;safety-in-raft&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#safety-in-raft&quot; aria-label=&quot;safety in raft permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Safety in RAFT&lt;/h3&gt;
&lt;p&gt;The rewriting of logs seems potentially dangerous. How can we be sure that we do not rewrite an entry that has already been applied to the state machine? If someone sends a &lt;code class=&quot;language-text&quot;&gt;PUT&lt;/code&gt; request and that is overwritten, the client will suddenly and inexplicably find his key absent from the database, which is unacceptable behavior. We need to add one more feature to RAFT leader election to ensure that safety is maintained. &lt;/p&gt;
&lt;p&gt;Essentially, what we want is that every leader who gets elected should have a log that contains all the committed entries. Recall that committed entries are those that have already been applied to the state machine. The need for this is evident, if the newly elected leader did not contain all the committed entries, then it would overwrite logs such that some committed entries would be deleted. RAFT uses the voting process to prevent a candidate from winning an election unless its log contains all the committed entries. A candidate must contact a majority of the servers to get elected, and it will only recieve a vote if its log is atlease as up-to-date as the server it is requesting a vote from. This is where the importance of the majority comes in. We know that in order for an entry to be committed, it has to be replicated on a majority of the servers. Therefore, the candidate will need to receive at least one vote from a server that has a log that contains all the committed entries, implying that the candidate will have a log at least as up-to-date as the server in question. This means that all elected leaders will contain &lt;strong&gt;all&lt;/strong&gt; committed entries, and we will never find an unsafe scenario. &lt;/p&gt;
&lt;p&gt;For a more formal proof of the Safety property, see the RAFT paper.&lt;/p&gt;
&lt;h2 id=&quot;zooming-out-and-applications&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#zooming-out-and-applications&quot; aria-label=&quot;zooming out and applications permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Zooming Out and Applications&lt;/h2&gt;
&lt;p&gt;We will zoom out and now look at how clients interact with a RAFT cluster. Clients of RAFT are supposed to send all their requests to the leader. On start-up, it connects to a randomly chosen server in the cluster. If this is not the leader, the client’s request will be rejected and the server will send it information about the most recent leader it has heard from. The client will then retry with the leader. If the leader crashes before returning a response, or before accepting the request, the client request times out, and it will then try again with randomly chosen servers. &lt;/p&gt;
&lt;p&gt;Once a client has found the leader, it will send it a request for some operation. The leader will append this request to the log, and then attempt to replicate it on a majority of the servers. If it’s successful in doing so, i.e. more than half of the &lt;strong&gt;AppendEntries&lt;/strong&gt; RPCs are successful (return true), it will apply the command to its own state and reply to the client. If the leader fails during this process, as mentioned above, the client will time-out and retry. If the leader is unable to replicate this on a majority of servers, the request will not get completed. The client will similarly time-out and retry. &lt;/p&gt;
&lt;p&gt;RAFT’s clean and understandable design, consisting of only two main RPCs, makes it an attractive choice for any company looking for a way to maintain a replicated state machine. This is especially true for database and data storage companies, like &lt;a href=&quot;https://www.cockroachlabs.com/docs/stable/architecture/replication-layer.html&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;CockroachDB&lt;/a&gt; and &lt;a href=&quot;https://www.mongodb.com/presentations/replication-election-and-consensus-algorithm-refinements-for-mongodb-3-2&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;MongoDB&lt;/a&gt;. RAFT is used to manage their replication layers, which is a layer of abstraction that handles storing multiple copies of the data to provide redundancy. Hashicorp’s &lt;a href=&quot;https://www.hashicorp.com/products/consul&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Consul&lt;/a&gt; also uses RAFT to maintain system reliability. Messaging solutions such as &lt;a href=&quot;https://www.rabbitmq.com/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;RabbitMQ&lt;/a&gt; use it as well. &lt;/p&gt;
&lt;h2 id=&quot;conclusion&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#conclusion&quot; aria-label=&quot;conclusion permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We thus saw how RAFT maintains a distributed log amongst multiple machines. RAFT is similar to Paxos (a very widely used consensus algorithm) in terms of performance and guarantees, but is a lot more understandable and implementable (try implementing a distributed version of RAFT if you want, message the distributed-systems channel on Slack for tips on how to get started). It also touches on a lot of topics that are core to distributed systems such as fault tolerance, a leader-follower model, RPCs for communication, heartbeats and consensus. &lt;/p&gt;
&lt;h2 id=&quot;useful-references&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#useful-references&quot; aria-label=&quot;useful references permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Useful References&lt;/h2&gt;
&lt;p&gt;If you want to dive deeper into RAFT, read the original &lt;a href=&quot;https://raft.github.io/raft.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;RAFT paper&lt;/a&gt;. There is also this &lt;a href=&quot;https://www.youtube.com/watch?v=vYp4LYbnnW8&amp;#x26;feature=youtu.be&amp;#x26;ab_channel=DiegoOngaro&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;talk&lt;/a&gt; by Professor John Ousterhout, one of the authors of the original paper. This &lt;a href=&quot;https://raft.github.io/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;website&lt;/a&gt; also contains more talks and visualizations about RAFT. Of course, if you have any specific questions, feel free to post in the distributed-channels Slack. &lt;/p&gt;</content:encoded></item></channel></rss>